{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 隐含马尔科夫链 解决 拼音转文字 问题\n",
    "\n",
    "## 什么是马尔科夫链\n",
    "\n",
    "简单的假设，随机过程中各个状态 $ S_t $的概率分布，只与它前一个状态 $ S_t-1 $有关，即$ P(S_t|S_1, S_2, ..., S_t-1)=P(S_t|S_t-1)$。\n",
    "\n",
    "比如说，对于天气预报，硬性的假设，今天的气温只和昨天有关而与前天无关。这种假设虽然未必适应所有的应用，但是相比较今天气温和多种因素相关来说，提出了一种近似解。这种假设就成为马尔科夫假设。这个随机过程称为马尔科夫链。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是隐含马尔科夫模型 (HMM, Hidden Markov Models)\n",
    "\n",
    "隐含马尔科夫模型是马尔科夫模型的一个扩展：任一时刻$t$的状态$S_t$是不可见的。所以观察者是没有办法通过观察到一个序列$S_1, S_2, ..., S_t$来推测转移概率等参数。但是隐含马尔科夫模型在每个时刻都会产生一个$O_t$，而且$O_t$和$S_t$相关**且**仅和$S_t$相关。这个被称为独立输出假设。其中隐含的状态 $S_1, S_2, ..., S_t$ 是一个典型的马尔科夫链，所以被称为隐含马尔科夫模型。\n",
    "\n",
    "![](./image/hmm1.jpg)\n",
    "\n",
    "基于马尔科夫假设和独立输出假设，我们可以计算出某个特定的状态序列$S_1, S_2, ..., S_t$产生输出符号$O_1, O_2, ..., O_t$的概率。\n",
    "\n",
    "$$ P(S_1, S_2, ..., S_t,O_1, O_2, ..., O_t) = \\prod_t P(S_t|S_t-1)*P(O_t|S_t)   \\qquad(1)$$ \n",
    "\n",
    "在通信中，接受端$O_1, O_2, ..., O_t$来推测信号源发送的信息$S_1, S_2, ..., S_t$，只需要从所有的源信息中找到最有可能产生观测信号的那一个信息。用概率论的语言来描述，就是在已知$O_1, O_2, ..., O_t$，求得领条件概率 $P(S_1, S_2, ..., S_t|O_1, O_2, ..., O_t)$达到最大值那个信息串$S_1, S_2, ..., S_t$,即 \n",
    "\n",
    "$$S_1, S_2, ..., S_t=ArgMax_{all S_1, S_2, ...} P(S_1, S_2, ..., S_t|O_1, O_2, ..., O_t)  \\qquad(2)$$  \n",
    "\n",
    "而上面的公式等价于\n",
    "\n",
    "$$P(O_1, O_2, ..., O_t|S_1, S_2, ..., S_t)*P(S_1, S_2, ..., S_t) = \\prod_t P(O_t|S_t) * \\prod_t P(S_t|S_t-1) \\qquad(3)$$ \n",
    "\n",
    "这样正好得到公式（1）。如何找到（2）公式的最大值，进而识别出句子$S_1, S_2, ..., S_t$，可以利用*维特比算法（Viterbi Algorithm）*，这个后面再说。\n",
    "\n",
    "针对不同的应用，P(S_1, S_2, ..., S_t|O_1, O_2, ..., O_t)的名称也不相同，*语音识别*中被称为“声学模型（Acoustic Model）”，在*机器翻译*中被称为“翻译模型”,在 *拼写校正*中是“纠错模型（Correction Model）”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用隐含马尔科夫模型解决问题\n",
    "\n",
    "用户输入拼音序列，就是一个可以观察到的序列，而拼音对应的汉字则可以看作是一个不可见的序列，要求解最大值那个信息串$S_1, S_2, ..., S_t$，使用隐含马尔科夫模型就可以解决问题。\n",
    "\n",
    "### 几个重要的参数\n",
    "\n",
    "在拼音转汉字中我们重新列出几个重要的参数：\n",
    "\n",
    "* V 是所有可能的观测的集合, 其中 M 是可能的观测数，对应所有可能的拼音的状态数量；\n",
    "\n",
    "* Q 是所有可能的隐藏状态的集合, 其中 N 是可能的状态数， 对应所有可能的汉字的状态数量；\n",
    "\n",
    "* I 是长度为 T 的隐藏状态序列， 对应用户输入的拼音序列对应的汉字序列；\n",
    "\n",
    "* O 是对应的观测序列， 对应用户输入的拼音序列；\n",
    "\n",
    "HMM 模型可以使用一个三元组来刻画 $\\lambda = (\\pi, A, B)$\n",
    "\n",
    "其中：\n",
    "\n",
    "* A是**隐藏状态转移概率（Transition Probability）分布**，在拼音转汉字的过程中就是句子中汉字到汉字的转移概率。\n",
    "\n",
    "* B是**观察状态到隐藏状态的生成概率（Generation Probability）**，在拼音转汉字的过程中就是拼音到汉字的发射概率。\n",
    "\n",
    "* $\\pi$ **隐藏状态初始的概率**，即每个汉字的初始概率。\n",
    "\n",
    "### 解决问题\n",
    "\n",
    "有了上述的一些基本参数，我们可以有一个初步的方案：\n",
    "\n",
    "1. 可以从搜索日志中抽取一批中文搜索词，再结合已有的词典，将对应中文转换为对应的拼音序列\n",
    "\n",
    "2. 对所有的词按照单字分词，并统计各个词出现的频率，即可作为初始化概率$\\pi$ \n",
    "\n",
    "3. 统计每个拼音对应汉字以及各自出现的次数，就可以得到观察概率分布B\n",
    "\n",
    "4. 最后统计每个汉字后面出现的汉字的次数，以此作为隐藏状态的转移概率分布A\n",
    "\n",
    "经过以上步骤之后，就得到一个隐马尔科夫模型 $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维特比算法\n",
    "\n",
    "维特比算法是一个特殊但应用最广的动态规划算法。在求解隐马尔科夫、条件随机场的预测以及seq2seq模型概率计算等问题中均用到了该算法，即对状态序列进行预测时用到。利用动态规划，可以解决任何一个图中的最短路径问题。而维特比算法是针对一个特殊的图-篱笆网了（Lattice）的有向图最短路径问题而提出来的。它之所以重要，是因为凡是使用隐马尔科夫模型描述的问题都可以用它解码，包括当前的数字通信、语音识别、机器翻译、拼音转汉字、分词等。\n",
    "\n",
    "\n",
    "假定用户（盲打时）输入的拼音是$y_1, y_2, ..., y_n$，对应的汉字是$x_1, x_2, ..., x_n$，可以用下图描述这样的一个过程：\n",
    "\n",
    "![](./image/viterbi1.png)\n",
    "\n",
    "这是一个相对简单的隐含马尔科夫链，这个马尔科夫链的每个状态的输出都是固定的，但是每一个状态的值是可变的。比如输入“zhong”的字可以是“中”，“钟”等多个字。我们不妨抽象一下，用符号$x_ij$表示状态$x_i$的第$j$个可能的值。如果按照不同的值展开，就得到了下面这个篱笆网络(Lattice):\n",
    "\n",
    "![](./image/viterbi2.png)\n",
    "\n",
    "$$x_1, x_2, ..., x_n=ArgMax_{all x_1, x_2, ...} P(x_1, x_2, ..., x_n|y_1, y_2, ..., y_n)=ArgMax_{all x_1, x_2, ...}\\prod_{i=1}^N P(y_i|x_i)*p(x_i|x_{i-1})  \\qquad(4)$$  \n",
    "\n",
    "如果按照（4）计算从第一个状态到最后一个状态找到最可能的路径，这样的路径组合数会跟序列状态数呈指数型增长。汉语中每个无声调的拼音对应13个左右的汉字，那么组合数为$13^{10} ~ 5*10^{14}$。假定计算每条路径概率需要20次乘法，就是$10^16$次计算。按照具体计算机每秒处理$10^11$次的计算，也许有大约计算一天的时间，因此使用穷举法肯定不合适。而维特比算法是一个和状态数目成正比的算法。\n",
    "\n",
    "![](./image/viterbi3.png)\n",
    "\n",
    "简单来说维特比算法可以有：**假定当我们从状态i进入到状态i+1时，从起点S到状态i上各个节点的最短路径已经找到了，并且记录在这些节点上。那么在计算从起点S到第i+1状态的某个节点的最短路径时，只需要考虑从S到前一个状态i所以的k个节点的最短路径，以及从这k个节点到$x_{i+1}$,j的距离即可。**\n",
    "\n",
    "这样每一步计算的复杂度都和相邻两个状态$S_i$和$S_{i+1}$格子节点数目$n_i, n_{i+1}$的乘积成正比，即$O(n_i*n_{i+1})$。如果隐含马尔科夫链中的节点最多有D个节点，网格长度为N，那么维特比算法的复杂度为$O(N*D^2)$。\n",
    "\n",
    "回到之前输入法的问题上，计算量基本上是$13*10*10=1690 \\approx 10^3$，这个和$10^16$有着天壤之别。那么无论在语音识别还是打字中，输入都是按照流的方式进行的，解码基本上是实时的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现\n",
    "\n",
    "基于上面的知识点，我们可以先实现一个简单的版本。在此之前通过对新闻等文本，制作了一些拼音转汉字的序列和频率、拼音序列和频率、一些词组的序列和频率等制作了一个简单的词典文件放置在data\\下，从而实现了在一定文本下的隐藏状态转移矩阵、隐藏状态初始状态、隐藏状态到观察状态的发射矩阵。\n",
    "\n",
    "当然再简单点，可以手动列出一些mapping直接用于测试，如：\n",
    "\n",
    "\\# 隐藏状态转移矩阵\n",
    "\n",
    "trans_prob = {\"周杰\":0.9,\n",
    "              \"周姐\":0.1,\n",
    "              \"周洁\":0.3,\n",
    "              \"杰伦\":0.8,\n",
    "              \"结论\":0.7\n",
    "              }\n",
    "\n",
    "\\# 隐藏状态初始状态\n",
    "\n",
    "pi = {\n",
    "    \"周\":0.5,\n",
    "    \"粥\":0.3,\n",
    "    \"杰\":0.5,\n",
    "    \"姐\":0.4,\n",
    "    \"节\":0.2,\n",
    "    \"结\":0.3,\n",
    "    \"轮\":0.1,\n",
    "    \"伦\":0.5,\n",
    "    \"论\":0.5,\n",
    "}\n",
    "\n",
    "\\# 隐藏状态到观察状态的发射矩阵\n",
    "\n",
    "emit_probs = {\n",
    "    \"周zhou\":0.5,\n",
    "    \"粥zhou\":0.1,\n",
    "    \"姐jie\":0.1,\n",
    "    \"节jie\":0.1,\n",
    "    \"结jie\":0.2,\n",
    "    \"杰jie\":0.2,\n",
    "    \"轮lun\":0.1,\n",
    "    \"伦lun\":0.3,\n",
    "    \"论lun\":0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.common import *\n",
    "\n",
    "\n",
    "pinyin2word = get_symbol_list()\n",
    "\n",
    "word2id = word2id()\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "pinyin2id = pinyin2id()\n",
    "id2pinyin = dict(zip(pinyin2id.values(), pinyin2id.keys()))\n",
    "\n",
    "# 隐藏状态转移矩阵\n",
    "trans_total_usage, trans_prob = get_hidden_status_trans_probs()\n",
    "# min_trans_prob = 0.1 * float(1) / float(trans_total_usage)\n",
    "\n",
    "# 隐藏状态初始矩阵\n",
    "hidden_status_total_usage, pi = get_hidden_status_init_probs()\n",
    "# 没有出现在pi矩阵的字给其设置一个初始的概率\n",
    "min_word_prob = 0.1 * float(1) / float(hidden_status_total_usage)\n",
    "\n",
    "# 隐藏状态到观察状态的发射矩阵\n",
    "emit_probs = get_hidden_to_observer_emit_prob(get_symbol_list())\n",
    "\n",
    "\n",
    "def get_word_in_pi_prob(word_id):\n",
    "    '''\n",
    "    返回某一个隐藏状态（汉字）在初始矩阵的概率，\n",
    "    如果没有则为min_word_prob\n",
    "    :param word_id:\n",
    "    :return:\n",
    "    '''\n",
    "    word = id2word[word_id]\n",
    "    if word in pi.keys():\n",
    "        return pi.get(word)\n",
    "    return min_word_prob\n",
    "\n",
    "\n",
    "def get_pinyin_word_emit_prob(pinyin_word):\n",
    "    '''\n",
    "    返回一个隐藏状态到观察状态（字+拼音）组合在发射矩阵的概率，\n",
    "    如果没有则为min_word_prob\n",
    "    :param pinyin_word:\n",
    "    :return:\n",
    "    '''\n",
    "    if pinyin_word in emit_probs.keys():\n",
    "        return emit_probs[pinyin_word]\n",
    "    return min_word_prob\n",
    "\n",
    "\n",
    "def viterbi(word_list, pinyin_list, n):\n",
    "    \"\"\"\n",
    "    维特比算法求解最大路径问题\n",
    "    :param word_list:   每个拼音对应的隐藏状态矩阵\n",
    "    :param n:   可能观察到的状态数， 对应为汉字数量\n",
    "    :param id2word:    id到汉字的映射\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    T = len(word_list)  # 观察状态的长度\n",
    "    delta = np.zeros((T, n))  # 转移值\n",
    "    psi = np.zeros((T, n), dtype=int)  # 转移下标值\n",
    "\n",
    "    # 初始化第一个字符的隐藏初始状态概率， 设置为每个词在词典中的单独出现的概率\n",
    "    words = word_list[0]\n",
    "    for i, w in enumerate(words):\n",
    "        delta[0][i] = get_word_in_pi_prob(w)\n",
    "\n",
    "    # 动态规划计算\n",
    "    for idx in range(1, T):\n",
    "        words = word_list[idx]  # 第T时刻所有可能出现的字的集合\n",
    "        for i in range(len(words)):\n",
    "            max_value = 0\n",
    "            pre_words = word_list[idx-1]\n",
    "\n",
    "            last_index = 0\n",
    "            for j in range(len(pre_words)):\n",
    "                tmp_key = id2word[pre_words[j]] + id2word[words[i]]  # 中国/钟国/忠国\n",
    "                # 获得转移概率，如果不存在则设置为0\n",
    "                if tmp_key in trans_prob.keys():\n",
    "                    prob = trans_prob[tmp_key]\n",
    "                else:\n",
    "                    prob = 0\n",
    "\n",
    "                # 前一时刻的字观察状态到隐藏状态的概率 * 转移概率\n",
    "                tmp_value = delta[idx-1][j] * prob\n",
    "                if tmp_value > max_value:\n",
    "                    max_value = tmp_value\n",
    "                    last_index = j\n",
    "\n",
    "            # 计算观察状态到隐藏状态的概率\n",
    "            tmp_pw_key = id2word[words[i]] + pinyin_list[idx]  # 国guo2\n",
    "            emit_prob = get_pinyin_word_emit_prob(tmp_pw_key) * max_value  # 观察状态到隐藏状态的概率 * 前一时刻所有字和当前字组合的最大概率\n",
    "\n",
    "            delta[idx][i] = emit_prob\n",
    "            psi[idx][i] = last_index  # 保存当前字的前一时刻 和当前字组合的最大概率 的下标值\n",
    "\n",
    "    prob = 0\n",
    "    path = np.zeros(T, dtype=int)\n",
    "    path[T-1] = 1\n",
    "\n",
    "    # 获取最大的转移值\n",
    "    desc_word_id = []\n",
    "    for i in range(n):\n",
    "        if prob < delta[T-1][i]:\n",
    "            prob = delta[T-1][i]\n",
    "            path[T-1] = i\n",
    "            desc_word_id.append(word_list[T-1][i])\n",
    "\n",
    "    # 最优路径回溯\n",
    "    for t in range(T-2, -1, -1):\n",
    "        last_index = psi[t+1][path[t+1]]\n",
    "        path[t] = last_index\n",
    "        desc_word_id.append(word_list[t][last_index])\n",
    "\n",
    "    final_word = \"\"\n",
    "    for id in reversed(desc_word_id):\n",
    "        final_word += id2word[id]\n",
    "\n",
    "    return final_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国人民\n",
      "明天高考\n",
      "后天就要出成系了\n",
      "希望你后天高考顺利\n",
      "我明天要吃苹果\n",
      "你的报价太高了\n",
      "勤劳勇敢\n",
      "新知度明名\n",
      "今天传了好多前\n",
      "你是否有好多问好\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    ['zhong1', 'guo2', 'ren2', 'min2'],\n",
    "    ['ming2', 'tian1', 'gao1', 'kao3'],\n",
    "    ['hou4', 'tian1', 'jiu4', 'yao4', 'chu1', 'cheng2', 'ji4', 'le5'],\n",
    "    ['xi1', 'wang4', 'ni3', 'hou4', 'tian1', 'gao1', 'kao3', 'shun4', 'li4'],\n",
    "    ['wo3', 'ming2', 'tian1', 'yao4', 'chi1', 'ping2', 'guo3'],\n",
    "    ['ni3', 'de5', 'bao4', 'jia4', 'tai4', 'gao1', 'le5'],\n",
    "    ['qin2', 'lao2', 'yong3', 'gan3'],\n",
    "    ['xin1', 'zhi1', 'du4', 'ming2'],\n",
    "    ['jin1', 'tian1', 'zhuan4', 'le5', \"hao3\", 'duo1', 'qian2'],\n",
    "    ['ni3', 'shi4', 'fou3', 'you3', \"hao3\", 'duo1', 'wen4', 'hao4']\n",
    "]\n",
    "\n",
    "for pinyin_list in tests:\n",
    "    word_id_list = []\n",
    "    n = 0\n",
    "    for i, single_pinyin in enumerate(pinyin_list):\n",
    "        single_pinyin_words = pinyin2word[single_pinyin]\n",
    "        if n < len(single_pinyin_words):\n",
    "            n = len(single_pinyin_words)\n",
    "        word_id_list.append([word2id[single_word] for single_word in single_pinyin_words])\n",
    "\n",
    "    words = viterbi(word_id_list, pinyin_list, n)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从实现结果上看，在语料内容不多的情况下，很快的能得出这样的结果还是相当不错了。但是更多的语料该怎么实现呢？新发现的词又如何加载呢？每次加载内存又会否可以承担呢？上面使用2n-gram的统计模型是否有更好的n-gram模型呢？这些后面再来讲解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
