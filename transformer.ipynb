{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型搭建\n",
    "\n",
    "![](./image/self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Input组件\n",
    "\n",
    "![](./image/input1.png)\n",
    "\n",
    "下面市县各个功能的组件\n",
    "\n",
    "### Norm Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "\n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A t\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)  # 计算 inputs的均值和方差\n",
    "        beta = tf.Variable(tf.zeros(params_shape))\n",
    "        gama = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
    "        \n",
    "        outputs = gama *normalized + beta\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "**embedding将大型稀疏向量转换为保留语义关系的低维空间。**embedding是一种矩阵，其中每列是与词汇表中的item对应的向量。 要获取单个词汇item的稠密向量，就检索与该item对应的列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(inputs, \n",
    "              vocab_size, \n",
    "              num_units, \n",
    "              zero_pad=True, \n",
    "              scale=True,\n",
    "              scope=\"embedding\", \n",
    "              reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "\n",
    "    For example,\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]\n",
    "    ```\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable(name='lookup_table',\n",
    "                                      dtype=tf.float32,\n",
    "                                      shape=[vocab_size, num_units],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)    \n",
    "        \n",
    "        if scale:\n",
    "                outputs = outputs * (num_units ** 0.5) \n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# inputs = tf.to_int32(np.array([[1]]))\n",
    "# outputs = embedding(inputs, 50, 2, zero_pad=False)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Layer\n",
    "\n",
    "![](./image/multihead1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(key_emb,\n",
    "                        que_emb,\n",
    "                        queries, \n",
    "                        keys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(key_emb, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "        \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "   \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "         \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(que_emb, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "两层全连接，用卷积模拟加速运算，也可以使用dense层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                 \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label_smoothing\n",
    "\n",
    "对于训练有好处，将0变为接近零的小数，1变为接近1的数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```\n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型\n",
    "\n",
    "模型结构如下：\n",
    "![](./image/model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "class TransformerModel():\n",
    "    def __init__(self, is_training=True):\n",
    "        tf.reset_default_graph()\n",
    "        self.hidden_units = arg.hidden_units\n",
    "        self.input_vocab_size = arg.input_vocab_size\n",
    "        self.label_vocab_size = arg.label_vocab_size\n",
    "        self.num_heads = arg.num_heads\n",
    "        self.num_blocks = arg.num_blocks\n",
    "        self.max_length = arg.max_length\n",
    "        self.lr = arg.lr\n",
    "        self.dropout = arg.dropout_rate\n",
    "        \n",
    "        # input\n",
    "        self.x = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.de_inp = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        \n",
    "        # Encoder\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            # embedding\n",
    "            self.en_emb = embedding(self.x, vocab_size=self.input_vocab_size, num_units=self.hidden_units, scale=True, scope=\"enc_embed\")\n",
    "            self.enc = self.en_emb + embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
    "                                          vocab_size=self.max_length,num_units=self.hidden_units, zero_pad=False, scale=False,scope=\"enc_pe\")\n",
    "#             pdb.set_trace()\n",
    "\n",
    "            # Dropout\n",
    "            self.enc = tf.layers.dropout(self.enc,rate=self.dropout, training=tf.convert_to_tensor(self.is_training))\n",
    "\n",
    "            # Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks-{}\".format(i)):\n",
    "                    # Multihead Attention\n",
    "                    self.enc = multihead_attention(key_emb=self.en_emb,\n",
    "                                                   que_emb=self.en_emb,\n",
    "                                                  queries=self.enc,\n",
    "                                                  keys=self.enc,\n",
    "                                                  num_units=self.hidden_units,\n",
    "                                                  dropout_rate=self.dropout,\n",
    "                                                  is_training=self.is_training,\n",
    "                                                  causality=False)\n",
    "\n",
    "            # Feed Forward\n",
    "            self.enc = feedforward(self.enc, num_units=[4*self.hidden_units, self.hidden_units])\n",
    "        \n",
    "        # Decoder\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # embedding\n",
    "            self.de_emb = embedding(self.de_inp, vocab_size=self.label_vocab_size, num_units=self.hidden_units, scale=True, scope=\"dec_embed\")\n",
    "            self.dec = self.de_emb + embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.de_inp)[1]), 0), [tf.shape(self.de_inp)[0], 1]),\n",
    "                                          vocab_size=self.max_length,num_units=self.hidden_units, zero_pad=False, scale=False,scope=\"dec_pe\")\n",
    "\n",
    "            ## Multihead Attention ( self-attention)\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    self.dec = multihead_attention(key_emb = self.de_emb,\n",
    "                                                   que_emb = self.de_emb,\n",
    "                                                   queries=self.dec, \n",
    "                                                    keys=self.dec, \n",
    "                                                    num_units=self.hidden_units, \n",
    "                                                    num_heads=self.num_heads, \n",
    "                                                    dropout_rate=self.dropout,\n",
    "                                                    is_training=self.is_training,\n",
    "                                                    causality=True,\n",
    "                                                    scope='self_attention')\n",
    "\n",
    "            ## Multihead Attention ( vanilla attention)\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    self.dec = multihead_attention(key_emb = self.en_emb,\n",
    "                                                   que_emb = self.de_emb,\n",
    "                                                   queries=self.dec, \n",
    "                                                    keys=self.enc, \n",
    "                                                    num_units=self.hidden_units, \n",
    "                                                    num_heads=self.num_heads, \n",
    "                                                    dropout_rate=self.dropout,\n",
    "                                                    is_training=self.is_training,\n",
    "                                                    causality=True,\n",
    "                                                    scope='vanilla_attention') \n",
    "\n",
    "            ### Feed Forward\n",
    "            self.outputs = feedforward(self.dec, num_units=[4*self.hidden_units, self.hidden_units])\n",
    "        \n",
    "        # 最终线性投影\n",
    "        self.logits = tf.layers.dense(inputs=self.outputs, units=self.label_vocab_size, activation=None)\n",
    "        self.preds = tf.to_int32(tf.arg_max(self.logits, -1))\n",
    "        self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "        tf.summary.scalar('acc', self.acc)\n",
    "        \n",
    "        if is_training:\n",
    "            #loss\n",
    "            self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=self.label_vocab_size))\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
    "            self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "        \n",
    "            # Training Scheme\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "            self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "            \n",
    "            # Summary \n",
    "            tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "            self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "读取数据，然后英文保存inputs，中文保存outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5161434/5161434 [00:23<00:00, 222167.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "with open(file=\"H:\\\\PycharmProjects\\\\dataset\\\\translation2019zh\\\\translation2019zh_train.json\", mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        dic = json.loads(line)\n",
    "        inputs.append(dic['english'].replace(',',' ,').lower())\n",
    "        outputs.append(dic['chinese'])\n",
    "        \n",
    "train_data_num_start = 60000\n",
    "train_data_num_end = train_data_num_start + 50000\n",
    "\n",
    "inputs = inputs[train_data_num_start: train_data_num_end]\n",
    "outputs = outputs[train_data_num_start: train_data_num_end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the engine has been tuned with an ess vt2-600 supercharger setup , chosen for its copious power , race-proven performance , and reliability.', \"great work! keep it up! don't stop working.\", 'this means helping your child accept and learn from their mistakes or misbehaviour and supporting swis to ensure the good educational and personal progress of your child.', \"but the cosmodrome is just a start for russia's new space ambitions.\", 'be anchored in the bay where all men ride.', 'the bottom of lpg fractionator is separated into two spaces by baffle.', 'blindy had come up opposite us and we smelled him and turned around.', 'if we drill holes in strategic locations , we compromise the load-carrying capacity of the wall。', 'he removes the drill bit from his pocket and marks the floor. michael slides over to a yellow scaffolding beam while he lifts up his left sleeve.', 'make sure you try out different crafting stations , some may be better suited for particular recipes or trade skills than others.']\n",
      "['该引擎已经被调整与斯洛文尼亚就业服务局VT2 - 600增压器设置，其荧光棒权力，赛道上经过验证的性能选择，和可靠性。', '做得真棒！再接再厉哦！别停下前进的脚步。', '这意味着帮助孩子学会承认错误并从错误中学习； 支持学校有助于孩子个人发展的良好教育。', '但航天器发射场只是俄罗斯新的空间雄心的开始。', '才在人尽可泊的港湾里抛锚。', '气体分馏装置的脱乙烷塔底设有隔板，将塔釜分为两个空间。', '这时盲汉已经走到我们对面，我们闻到他身上的气味，便转过身去。', '如果我们在关键位置钻孔，墙的承重强度就会降低。', '他从口袋里拿出了钻头，在地上做了个记号，接着走到一个黄色的脚手架边并卷起他的左袖，用他上臂的纹身在脚手架上量出一个点，然后用钻头作了个记号。', '请去尝试著不同的工作桌，一些更适合的桌子可以给特别的配方或者是交易技术以及其他更多的东西等等。']\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:10])\n",
    "\n",
    "print(outputs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 英文分词\n",
    "\n",
    "英文只需要通过空格隔开就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 489750.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'engine',\n",
       "  'has',\n",
       "  'been',\n",
       "  'tuned',\n",
       "  'with',\n",
       "  'an',\n",
       "  'ess',\n",
       "  'vt2-600',\n",
       "  'supercharger',\n",
       "  'setup',\n",
       "  ',',\n",
       "  'chosen',\n",
       "  'for',\n",
       "  'its',\n",
       "  'copious',\n",
       "  'power',\n",
       "  ',',\n",
       "  'race-proven',\n",
       "  'performance',\n",
       "  ',',\n",
       "  'and',\n",
       "  'reliability.'],\n",
       " ['great', 'work!', 'keep', 'it', 'up!', \"don't\", 'stop', 'working.'],\n",
       " ['this',\n",
       "  'means',\n",
       "  'helping',\n",
       "  'your',\n",
       "  'child',\n",
       "  'accept',\n",
       "  'and',\n",
       "  'learn',\n",
       "  'from',\n",
       "  'their',\n",
       "  'mistakes',\n",
       "  'or',\n",
       "  'misbehaviour',\n",
       "  'and',\n",
       "  'supporting',\n",
       "  'swis',\n",
       "  'to',\n",
       "  'ensure',\n",
       "  'the',\n",
       "  'good',\n",
       "  'educational',\n",
       "  'and',\n",
       "  'personal',\n",
       "  'progress',\n",
       "  'of',\n",
       "  'your',\n",
       "  'child.'],\n",
       " ['but',\n",
       "  'the',\n",
       "  'cosmodrome',\n",
       "  'is',\n",
       "  'just',\n",
       "  'a',\n",
       "  'start',\n",
       "  'for',\n",
       "  \"russia's\",\n",
       "  'new',\n",
       "  'space',\n",
       "  'ambitions.'],\n",
       " ['be', 'anchored', 'in', 'the', 'bay', 'where', 'all', 'men', 'ride.'],\n",
       " ['the',\n",
       "  'bottom',\n",
       "  'of',\n",
       "  'lpg',\n",
       "  'fractionator',\n",
       "  'is',\n",
       "  'separated',\n",
       "  'into',\n",
       "  'two',\n",
       "  'spaces',\n",
       "  'by',\n",
       "  'baffle.'],\n",
       " ['blindy',\n",
       "  'had',\n",
       "  'come',\n",
       "  'up',\n",
       "  'opposite',\n",
       "  'us',\n",
       "  'and',\n",
       "  'we',\n",
       "  'smelled',\n",
       "  'him',\n",
       "  'and',\n",
       "  'turned',\n",
       "  'around.'],\n",
       " ['if',\n",
       "  'we',\n",
       "  'drill',\n",
       "  'holes',\n",
       "  'in',\n",
       "  'strategic',\n",
       "  'locations',\n",
       "  ',',\n",
       "  'we',\n",
       "  'compromise',\n",
       "  'the',\n",
       "  'load-carrying',\n",
       "  'capacity',\n",
       "  'of',\n",
       "  'the',\n",
       "  'wall。'],\n",
       " ['he',\n",
       "  'removes',\n",
       "  'the',\n",
       "  'drill',\n",
       "  'bit',\n",
       "  'from',\n",
       "  'his',\n",
       "  'pocket',\n",
       "  'and',\n",
       "  'marks',\n",
       "  'the',\n",
       "  'floor.',\n",
       "  'michael',\n",
       "  'slides',\n",
       "  'over',\n",
       "  'to',\n",
       "  'a',\n",
       "  'yellow',\n",
       "  'scaffolding',\n",
       "  'beam',\n",
       "  'while',\n",
       "  'he',\n",
       "  'lifts',\n",
       "  'up',\n",
       "  'his',\n",
       "  'left',\n",
       "  'sleeve.'],\n",
       " ['make',\n",
       "  'sure',\n",
       "  'you',\n",
       "  'try',\n",
       "  'out',\n",
       "  'different',\n",
       "  'crafting',\n",
       "  'stations',\n",
       "  ',',\n",
       "  'some',\n",
       "  'may',\n",
       "  'be',\n",
       "  'better',\n",
       "  'suited',\n",
       "  'for',\n",
       "  'particular',\n",
       "  'recipes',\n",
       "  'or',\n",
       "  'trade',\n",
       "  'skills',\n",
       "  'than',\n",
       "  'others.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [en.split(' ') for en in tqdm(inputs) if en != ',']\n",
    "\n",
    "inputs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词\n",
    "\n",
    "通过结巴分词进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/50000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\crixue\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.730 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:09<00:00, 5484.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['该',\n",
       "  '引擎',\n",
       "  '已经',\n",
       "  '被',\n",
       "  '调整',\n",
       "  '与',\n",
       "  '斯洛文尼亚',\n",
       "  '就业',\n",
       "  '服务局',\n",
       "  'VT2',\n",
       "  '-',\n",
       "  '600',\n",
       "  '增压器',\n",
       "  '设置',\n",
       "  '，',\n",
       "  '其',\n",
       "  '荧光棒',\n",
       "  '权力',\n",
       "  '，',\n",
       "  '赛道',\n",
       "  '上',\n",
       "  '经过',\n",
       "  '验证',\n",
       "  '的',\n",
       "  '性能',\n",
       "  '选择',\n",
       "  '，',\n",
       "  '和',\n",
       "  '可靠性',\n",
       "  '。'],\n",
       " ['做', '得', '真棒', '！', '再接再厉', '哦', '！', '别', '停下', '前进', '的', '脚步', '。'],\n",
       " ['这',\n",
       "  '意味着',\n",
       "  '帮助',\n",
       "  '孩子',\n",
       "  '学会',\n",
       "  '承认错误',\n",
       "  '并',\n",
       "  '从',\n",
       "  '错误',\n",
       "  '中',\n",
       "  '学习',\n",
       "  '；',\n",
       "  '支持',\n",
       "  '学校',\n",
       "  '有助于',\n",
       "  '孩子',\n",
       "  '个人',\n",
       "  '发展',\n",
       "  '的',\n",
       "  '良好',\n",
       "  '教育',\n",
       "  '。'],\n",
       " ['但', '航天器', '发射场', '只是', '俄罗斯', '新', '的', '空间', '雄心', '的', '开始', '。'],\n",
       " ['才', '在', '人', '尽', '可泊', '的', '港湾', '里', '抛锚', '。'],\n",
       " ['气体',\n",
       "  '分馏',\n",
       "  '装置',\n",
       "  '的',\n",
       "  '脱乙烷',\n",
       "  '塔底',\n",
       "  '设有',\n",
       "  '隔板',\n",
       "  '，',\n",
       "  '将',\n",
       "  '塔',\n",
       "  '釜',\n",
       "  '分为',\n",
       "  '两个',\n",
       "  '空间',\n",
       "  '。'],\n",
       " ['这时',\n",
       "  '盲汉',\n",
       "  '已经',\n",
       "  '走',\n",
       "  '到',\n",
       "  '我们',\n",
       "  '对面',\n",
       "  '，',\n",
       "  '我们',\n",
       "  '闻到',\n",
       "  '他',\n",
       "  '身上',\n",
       "  '的',\n",
       "  '气味',\n",
       "  '，',\n",
       "  '便',\n",
       "  '转过身',\n",
       "  '去',\n",
       "  '。'],\n",
       " ['如果',\n",
       "  '我们',\n",
       "  '在',\n",
       "  '关键',\n",
       "  '位置',\n",
       "  '钻孔',\n",
       "  '，',\n",
       "  '墙',\n",
       "  '的',\n",
       "  '承重',\n",
       "  '强度',\n",
       "  '就',\n",
       "  '会',\n",
       "  '降低',\n",
       "  '。'],\n",
       " ['他',\n",
       "  '从',\n",
       "  '口袋',\n",
       "  '里',\n",
       "  '拿出',\n",
       "  '了',\n",
       "  '钻头',\n",
       "  '，',\n",
       "  '在',\n",
       "  '地上',\n",
       "  '做',\n",
       "  '了',\n",
       "  '个',\n",
       "  '记号',\n",
       "  '，',\n",
       "  '接着',\n",
       "  '走',\n",
       "  '到',\n",
       "  '一个',\n",
       "  '黄色',\n",
       "  '的',\n",
       "  '脚手架',\n",
       "  '边',\n",
       "  '并',\n",
       "  '卷起',\n",
       "  '他',\n",
       "  '的',\n",
       "  '左袖',\n",
       "  '，',\n",
       "  '用',\n",
       "  '他',\n",
       "  '上臂',\n",
       "  '的',\n",
       "  '纹身',\n",
       "  '在',\n",
       "  '脚手架',\n",
       "  '上量',\n",
       "  '出',\n",
       "  '一个点',\n",
       "  '，',\n",
       "  '然后',\n",
       "  '用',\n",
       "  '钻头',\n",
       "  '作',\n",
       "  '了',\n",
       "  '个',\n",
       "  '记号',\n",
       "  '。'],\n",
       " ['请',\n",
       "  '去',\n",
       "  '尝试',\n",
       "  '著',\n",
       "  '不同',\n",
       "  '的',\n",
       "  '工作桌',\n",
       "  '，',\n",
       "  '一些',\n",
       "  '更',\n",
       "  '适合',\n",
       "  '的',\n",
       "  '桌子',\n",
       "  '可以',\n",
       "  '给',\n",
       "  '特别',\n",
       "  '的',\n",
       "  '配方',\n",
       "  '或者',\n",
       "  '是',\n",
       "  '交易',\n",
       "  '技术',\n",
       "  '以及',\n",
       "  '其他',\n",
       "  '更',\n",
       "  '多',\n",
       "  '的',\n",
       "  '东西',\n",
       "  '等等',\n",
       "  '。']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "outputs = [[char for char in jieba.cut(line) if char != ' '] for line in tqdm(outputs)]\n",
    "\n",
    "outputs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  生成字典\n",
    "\n",
    "将英文和中文映射为id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_cache(fname='encoder_vocab_mapping.txt'):\n",
    "    data = []\n",
    "    if(os.path.exists(fname)):\n",
    "        with open(fname, mode='r', encoding='utf-8') as r:\n",
    "            data = r.read().split(' ')\n",
    "    return data\n",
    "\n",
    "def save_vocab_cache(data, fname='encoder_vocab_mapping.txt'):\n",
    "    with open(fname, mode='w', encoding='utf-8') as w:\n",
    "        for word in data:\n",
    "            w.write(word + ' ')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████▉                                                             | 9820/50000 [00:24<01:46, 376.30it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "SOURCE_CODES = ['<PAD>']\n",
    "TARGET_CODES = ['<PAD>', '<GO>', '<EOS>']\n",
    "\n",
    "def get_vocab(data, init=['<PAD>'], cache_fname='encoder_vocab_mapping.txt'):\n",
    "    vocab = init\n",
    "    cache = get_vocab_cache(cache_fname)\n",
    "    if(len(cache) > 0):\n",
    "        vocab = cache\n",
    "    \n",
    "    for line in tqdm(data):\n",
    "        for word in line:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    return vocab\n",
    "\n",
    "encoder_vocab = get_vocab(inputs, init=SOURCE_CODES, cache_fname='encoder_vocab_mapping.txt')\n",
    "decoder_vocab = get_vocab(outputs, init=TARGET_CODES, cache_fname='decoder_vocab_mapping.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_vocab[:10])\n",
    "print(decoder_vocab[-20:-10])\n",
    "\n",
    "# save_vocab_cache(encoder_vocab, fname='encoder_vocab_mapping.txt')\n",
    "# save_vocab_cache(decoder_vocab, fname='decoder_vocab_mapping.txt')\n",
    "# get_vocab_cache(fname='encoder_vocab_mapping.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  数据生成器\n",
    "\n",
    "翻译系统训练所需要的数据形式，跟谷歌gnmt输入致，gnmt的原理可以参考：https://github.com/tensorflow/nmt\n",
    "大概是:\n",
    "\n",
    "* 编码器输入：I am a student\n",
    "* 解码器输入：(go) Je suis étudiant\n",
    "* 解码器输出：Je suis étudiant (end)\n",
    "\n",
    "即解码器输入起始部分有个开始符号，输出句尾有个结束符号。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = [[encoder_vocab.index(word) for word in line] for line in tqdm(inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_val_used = [[decoder_vocab.index(word) for word in line] for line in tqdm(outputs)]\n",
    "decoder_inputs = [[decoder_vocab.index('<GO>')] + mapping_index for mapping_index in tqdm(decoder_val_used)]\n",
    "decoder_targets = [mapping_index + [decoder_vocab.index('<EOS>')] for mapping_index in tqdm(decoder_val_used)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_inputs = [[decoder_vocab.index('<GO>')] + [decoder_vocab.index(word) for word in line] for line in tqdm(outputs)]\n",
    "# decoder_targets = [[decoder_vocab.index(word) for word in line] + [decoder_vocab.index('<EOS>')] for line in tqdm(outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_inputs[:20])\n",
    "print(decoder_targets[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_batch(encoder_inputs, decoder_inputs, decoder_targets, batch_size, batch_num):\n",
    "    batch_num -= 1\n",
    "    for k in (range(batch_num)):\n",
    "        start = k * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        en_inputs_batch = encoder_inputs[start: end]\n",
    "        de_inputs_batch = decoder_inputs[start: end]\n",
    "        de_targets_batch = decoder_targets[start: end]\n",
    "        \n",
    "        en_len = [len(line) for line in en_inputs_batch]\n",
    "        de_len = [len(line) for line in de_inputs_batch]\n",
    "        \n",
    "#         if len(en_len) == 0 or len(de_len) == 0:\n",
    "#             continue\n",
    "            \n",
    "        max_words_len = 64\n",
    "        max_en_len = max_words_len if max(en_len) > max_words_len else max(en_len)\n",
    "        max_de_len = max_words_len if max(de_len) > max_words_len else max(de_len)\n",
    "        \n",
    "        en_inputs_batch = pad_sequences(sequences=en_inputs_batch, maxlen=max_en_len, padding='post', dtype='int32', value=0)\n",
    "        de_inputs_batch = pad_sequences(sequences=de_inputs_batch, maxlen=max_de_len, padding='post', dtype='int32', value=0)\n",
    "        de_targets_batch = pad_sequences(sequences=de_targets_batch, maxlen=max_de_len, padding='post', dtype='int32', value=0)\n",
    "        yield en_inputs_batch, de_inputs_batch, de_targets_batch\n",
    "\n",
    "        \n",
    "batch_size = 4\n",
    "batch = get_batch(encoder_inputs, decoder_inputs, decoder_targets, batch_size, len(encoder_inputs) // batch_size)\n",
    "next(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hparams():\n",
    "    params = tf.contrib.training.HParams(\n",
    "        num_heads = 8,\n",
    "        num_blocks = 6,\n",
    "        # vocab\n",
    "        input_vocab_size = 50,\n",
    "        label_vocab_size = 50,\n",
    "        # embedding size\n",
    "        max_length = 100,\n",
    "        hidden_units = 128,\n",
    "        dropout_rate = 0.2,\n",
    "        lr = 0.000005)\n",
    "    return params\n",
    "\n",
    "arg = create_hparams()\n",
    "arg.input_vocab_size = len(encoder_vocab)\n",
    "arg.label_vocab_size = len(decoder_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "validate_ratio = 0.2\n",
    "\n",
    "encoder_inputs, decoder_inputs, decoder_targets = shuffle(encoder_inputs, decoder_inputs, decoder_targets)\n",
    "train_num = int(len(encoder_inputs) * (1 - validate_ratio))\n",
    "train_encoder_inputs = encoder_inputs[:train_num]\n",
    "val_encoder_inputs = encoder_inputs[train_num:]\n",
    "train_decoder_inputs = decoder_inputs[:train_num]\n",
    "val_decoder_inputs = decoder_inputs[train_num:]\n",
    "train_decoder_targets = decoder_targets[:train_num]\n",
    "val_decoder_targets = decoder_targets[train_num:]\n",
    "\n",
    "print(train_encoder_inputs[:3])\n",
    "print(train_decoder_inputs[:3])\n",
    "print(train_decoder_targets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "\n",
    "epochs = 35\n",
    "batch_size = 4\n",
    "\n",
    "model = TransformerModel()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=1)\n",
    "with tf.Session(config=config) as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    model_path = 'logs'\n",
    "    if(os.path.exists(os.path.join(model_path, 'TransformerModel.meta'))):\n",
    "        saver.restore(sess, \"logs\\TransformerModel\")\n",
    "\n",
    "    writer = tf.summary.FileWriter('tensorboard/train', tf.get_default_graph())\n",
    "    validation_writer = tf.summary.FileWriter('tensorboard/validate', tf.get_default_graph())\n",
    "    \n",
    "    for k in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_num = len(train_encoder_inputs) // batch_size\n",
    "        batch = get_batch(train_encoder_inputs, train_decoder_inputs, train_decoder_targets, batch_size, batch_num)\n",
    "        val_batch_num = len(val_encoder_inputs) // batch_size\n",
    "        val_batch = get_batch(val_encoder_inputs, val_decoder_inputs, val_decoder_targets, batch_size, val_batch_num)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for i in range(batch_num):\n",
    "            try:\n",
    "                encoder_input, decoder_input, decoder_target = next(batch)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            feed = {model.x: encoder_input, model.y: decoder_input, model.de_inp: decoder_target, model.is_training: True}\n",
    "            cost, _, acc = sess.run([model.mean_loss, model.train_op, model.acc], feed_dict=feed)\n",
    "            total_loss += cost\n",
    "            if (k * batch_num + i) % 10 == 0:\n",
    "                val_encoder_input, val_decoder_input, val_decoder_target = next(val_batch)\n",
    "                val_feed = {model.x: val_encoder_input, model.y: val_decoder_input, model.de_inp: val_decoder_target, model.is_training: False}\n",
    "                val_acc = sess.run([model.acc], feed_dict=val_feed)\n",
    "                \n",
    "                print(\"batch num :\", i,\", train average loss:\", total_loss/(i+1),\", acc:\", acc)\n",
    "                print(\"validation current acc:\", val_acc[0])\n",
    "                \n",
    "                rs = sess.run(merged, feed_dict=feed)\n",
    "                writer.add_summary(rs, k*batch_num+i)\n",
    "                \n",
    "                vs = sess.run(merged, feed_dict=val_feed)\n",
    "                validation_writer.add_summary(vs, k*batch_num+i)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('one epoch consume time:', (end_time-start_time), \"s\")\n",
    "        print('=== epoch:', k+1, ' average loss: ', total_loss/(batch_num+1), \" accuracy:\", acc)\n",
    "        print('-----**-----**-----**-----**-----**-----**-----**-----**-----**-----**-----')\n",
    "    \n",
    "    saver.save(sess=sess, save_path=os.path.join(model_path, 'TransformerModel'))\n",
    "    writer.close()\n",
    "    validation_writer.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试和推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先准备测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import jieba\n",
    "\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "with open(file=\"H:\\\\PycharmProjects\\\\dataset\\\\translation2019zh\\\\translation2019zh_valid.json\", mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        dic = json.loads(line)\n",
    "        test_inputs.append(dic['english'].replace(',',' ,').lower())\n",
    "        test_outputs.append(dic['chinese'])\n",
    "        \n",
    "test_data_ratio = 0.0125\n",
    "test_inputs_ori, test_outputs_ori = shuffle(test_inputs, test_outputs)\n",
    "test_data_num = int(len(test_inputs_ori) * test_data_ratio)\n",
    "test_inputs = test_inputs_ori[:test_data_num]\n",
    "test_outputs = test_outputs_ori[:test_data_num]\n",
    "test_inputs = [en.split(' ') for en in tqdm(test_inputs) if en != ',']\n",
    "test_outputs = [[char for char in jieba.cut(line) if char != ' '] for line in tqdm(test_outputs)]\n",
    "test_encoder_vocab = get_vocab(test_inputs, init=SOURCE_CODES, cache_fname='encoder_vocab_mapping.txt')\n",
    "test_decoder_vocab = get_vocab(test_outputs, init=TARGET_CODES, cache_fname='decoder_vocab_mapping.txt')\n",
    "test_encoder_inputs = [[encoder_vocab.index(word) for word in line] for line in tqdm(test_inputs)]\n",
    "test_decoder_val_used = [[test_decoder_vocab.index(word) for word in line] for line in tqdm(test_outputs)]\n",
    "test_decoder_inputs = [[test_decoder_vocab.index('<GO>')] + mapping_index for mapping_index in tqdm(test_decoder_val_used)]\n",
    "test_decoder_targets = [mapping_index + [test_decoder_vocab.index('<EOS>')] for mapping_index in tqdm(test_decoder_val_used)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(encoder_inputs, decoder_inputs, decoder_targets):\n",
    "    for i in range(len(encoder_inputs)):\n",
    "        yield np.array([encoder_inputs[i]]), np.array([decoder_inputs[i]]), np.array([decoder_targets[i]])\n",
    "\n",
    "test_batch = get_test_batch(test_encoder_inputs, test_decoder_inputs, test_decoder_targets)\n",
    "next(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "model_path = 'logs'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if(os.path.exists(os.path.join(model_path, 'TransformerModel.meta'))):\n",
    "        saver.restore(sess, \"logs\\TransformerModel\")\n",
    "        \n",
    "    test_acc_count = 0.0\n",
    "    for j in range(len(test_encoder_inputs)):\n",
    "        try:\n",
    "            test_encoder_input, test_decoder_input, test_decoder_target = next(test_batch)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "        de_inp = [[test_decoder_vocab.index('<GO>')]]\n",
    "        test_feed = {model.x: test_encoder_input, model.de_inp: np.array(de_inp), model.is_training: False}\n",
    "        preds = sess.run(model.preds, feed_dict=test_feed)\n",
    "#         pdb.set_trace()\n",
    "        if preds[0][-1] == test_decoder_vocab.index('<EOS>'):\n",
    "            break\n",
    "        de_inp[0].append(preds[0][-1]) \n",
    "        decoder_preds_words = ''.join([test_decoder_vocab[index] for index in de_inp[0][1:]])\n",
    "        \n",
    "        print(\"*****===*****===*****===*****===*****===*****===*****===*****\")\n",
    "        print(\"要求翻译的句子：\", test_inputs_ori[j])\n",
    "        print(\"预测翻译成：\",decoder_preds_words)\n",
    "        print(\"正确翻译为：\", test_outputs_ori[j])\n",
    "        print(\"*****===*****===*****===*****===*****===*****===*****===***** \\n\")\n",
    "        decoder_preds_words = ''\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
